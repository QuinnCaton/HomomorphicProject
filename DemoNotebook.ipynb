{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tenseal as ts\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import fingerprint_feature_extractor\n",
    "import fingerprint_enhancer\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minutiae(num_fingerprints, num_minutiae_per_fingerprint):\n",
    "    dataset = []\n",
    "    for i in range(num_fingerprints):\n",
    "        fingerprint = []\n",
    "        fingerprint_id = i+1\n",
    "        for _ in range(num_minutiae_per_fingerprint):\n",
    "            x = random.randint(0, 500) # x coord\n",
    "            y = random.randint(0, 500) # y coord\n",
    "            angle = random.randint(0, 360) # angle of minutiae\n",
    "            minutiae_type = random.randint(0, 1) #0 for ridge, 1 for bif\n",
    "            fingerprint.append((fingerprint_id, x, y, angle, minutiae_type)) # add to individual fingerprint data\n",
    "        dataset.append(fingerprint)\n",
    "    return dataset\n",
    "\n",
    "def save_to_csv(dataset, filename):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['ID', 'X', 'Y', 'Orientation', 'Type'])\n",
    "        for fingerprint in dataset:\n",
    "            for minutiae in fingerprint:\n",
    "                writer.writerow(minutiae)\n",
    "\n",
    "def encrypt_fingerprint_data(df, context):\n",
    "    encrypted_minutiae_dict = {}\n",
    "    for fingerprint_id in df['ID'].unique():\n",
    "        fingerprint_minutiae = df[df['ID'] == fingerprint_id][['X', 'Y', 'Orientation', 'Type']].values.flatten().tolist()\n",
    "        scaled_minutiae = [float(value) for value in fingerprint_minutiae]\n",
    "        encrypted_minutiae = ts.ckks_vector(context, scaled_minutiae)\n",
    "        encrypted_minutiae_dict[fingerprint_id]=encrypted_minutiae\n",
    "    return encrypted_minutiae_dict\n",
    "\n",
    "def find_matching_fingerprint_id(encrypted_query, encrypted_data, context):\n",
    "    min_distance = float('inf')\n",
    "    matching_id = None\n",
    "\n",
    "    for fingerprint_id, encrypted_minutiae in encrypted_data.items():\n",
    "        difference_vector = encrypted_query - encrypted_minutiae\n",
    "        distance_squared = difference_vector.dot(difference_vector)\n",
    "\n",
    "        distance_squared_decrypted = distance_squared.decrypt()\n",
    "        distance = np.sqrt(distance_squared_decrypted)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            matching_id = fingerprint_id\n",
    "\n",
    "    return matching_id\n",
    "\n",
    "def normalize_and_scale_minutiae(df):\n",
    "    # Example normalization - adjust based on your data's characteristics\n",
    "    df['locX'] = (df['locX'] / df['locX'].max())\n",
    "    df['locY'] = (df['locY'] / df['locY'].max())\n",
    "    #df['Orientation'] = df['Orientation'] / 360\n",
    "    # No change for 'Type' as it's categorical (0 or 1)\n",
    "    return df\n",
    "\n",
    "def pad_or_truncate_minutiae_list(minutiaeList, targetLength):\n",
    "    # Define dummy minutiae with impossible coordinates and orientation as placeholder\n",
    "    dummy_minutiae = fingerprint_feature_extractor.MinutiaeFeature(-1, -1, float('nan'), 'Dummy')\n",
    "    currentLength = len(minutiaeList)\n",
    "    \n",
    "    if currentLength > targetLength:\n",
    "        return minutiaeList[:targetLength]\n",
    "    else:\n",
    "        return minutiaeList + [dummy_minutiae] * (targetLength - currentLength)\n",
    "\n",
    "def process_images_from_folder(folder_path, csv_filepath, target_minutiae_count, enrollment = True):\n",
    "    headers = ['ID', 'Type', 'LocationX', 'LocationY', 'Orientation']\n",
    "    \n",
    "    with open(csv_filepath, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        ID = 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".tif\"):  # Check for TIFF images, adjust if using other formats\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = cv2.imread(img_path, 0)\n",
    "                out = fingerprint_enhancer.enhance_Fingerprint(img)\n",
    "                \n",
    "                # Process image to enhance and extract features, replace with your actual method\n",
    "                # out = fingerprint_enhancer.enhance_Fingerprint(img)\n",
    "                if(enrollment==False):\n",
    "                    cv2.imshow('enhanced_image', out);\n",
    "                    cv2.waitKey(0)\n",
    "                    FeaturesTerminations, FeaturesBifurcations = fingerprint_feature_extractor.extract_minutiae_features(\n",
    "                                    out, spuriousMinutiaeThresh=10, invertImage=False, showResult=True, saveResult=True)\n",
    "                else:\n",
    "                    FeaturesTerminations, FeaturesBifurcations = fingerprint_feature_extractor.extract_minutiae_features(\n",
    "                                    out, spuriousMinutiaeThresh=10, invertImage=False, showResult=False, saveResult=True)\n",
    "                \n",
    "                # Ensure a fixed number of minutiae\n",
    "                FeaturesTerminations = pad_or_truncate_minutiae_list(FeaturesTerminations, target_minutiae_count//2)\n",
    "                FeaturesBifurcations = pad_or_truncate_minutiae_list(FeaturesBifurcations, target_minutiae_count//2)\n",
    "\n",
    "                # Write termination features\n",
    "                for feature in FeaturesTerminations:\n",
    "                    writer.writerow([ID, 0, feature.locX, feature.locY, feature.Orientation])\n",
    "                \n",
    "                # Write bifurcation features\n",
    "                for feature in FeaturesBifurcations:\n",
    "                    writer.writerow([ID, 1, feature.locX, feature.locY, feature.Orientation])\n",
    "            ID+=1\n",
    "\n",
    "def encryptPrintsDatabase(df, context):\n",
    "    total_prints = len(pd.unique(df['ID']))\n",
    "    database = {}\n",
    "    for i in range(0, total_prints):\n",
    "        fingerprint_minutiae = df[df['ID'] == i][['LocationX', 'LocationY', 'Orientation', 'Type']].values.flatten().tolist()\n",
    "        scaled_minutiae = []\n",
    "\n",
    "        for value in fingerprint_minutiae:\n",
    "            try:\n",
    "                # Attempt to convert directly to float\n",
    "                converted_value = float(value)\n",
    "                if np.isnan(converted_value):\n",
    "                    converted_value = 0  # Replace NaN with 0 or another appropriate value\n",
    "                scaled_minutiae.append(converted_value)\n",
    "            except ValueError:\n",
    "                # If direct conversion fails, try evaluating as a list and then convert each element\n",
    "                try:\n",
    "                    list_values = ast.literal_eval(value)\n",
    "                    if isinstance(list_values, list):\n",
    "                        scaled_list_values = [float(v) if not np.isnan(float(v)) else 0 for v in list_values]  # Replace NaN within the list\n",
    "                        scaled_minutiae.extend(scaled_list_values)\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "                except ValueError:\n",
    "                    # If conversion still fails, handle or report the erroneous value\n",
    "                    print(f\"Could not convert value: {value}\")\n",
    "\n",
    "        val = 0\n",
    "        target_length = 1000\n",
    "\n",
    "        while len(scaled_minutiae) < target_length:\n",
    "            scaled_minutiae.append(val)\n",
    "        arr = np.array(scaled_minutiae)\n",
    "        enc = ts.ckks_vector(context, arr)\n",
    "        database[i] = enc\n",
    "    \n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_to_dataframe(ID, img_path, file_path, target_minutiae_count):\n",
    "    headers = ['ID', 'Type', 'LocationX', 'LocationY', 'Orientation']\n",
    "    \n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        out = fingerprint_enhancer.enhance_Fingerprint(img)\n",
    "        cv2.imshow('enhanced_image', out);\n",
    "        cv2.waitKey(0)\n",
    "        # Extract features\n",
    "        FeaturesTerminations, FeaturesBifurcations = fingerprint_feature_extractor.extract_minutiae_features(\n",
    "                        out, spuriousMinutiaeThresh=10, invertImage=False, showResult=True, saveResult=True)\n",
    "\n",
    "        FeaturesTerminations = pad_or_truncate_minutiae_list(FeaturesTerminations, target_minutiae_count//2)\n",
    "        FeaturesBifurcations = pad_or_truncate_minutiae_list(FeaturesBifurcations, target_minutiae_count//2)\n",
    "\n",
    "    # Write termination features\n",
    "        for feature in FeaturesTerminations:\n",
    "            writer.writerow([ID, 0, feature.locX, feature.locY, feature.Orientation])\n",
    "        \n",
    "        # Write bifurcation features\n",
    "        for feature in FeaturesBifurcations:\n",
    "            writer.writerow([ID, 1, feature.locX, feature.locY, feature.Orientation])\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def encryptSinglePrint(df, ID, context):\n",
    "    fingerprint_minutiae = df[df['ID'] == ID][['LocationX', 'LocationY', 'Orientation', 'Type']].values.flatten().tolist()\n",
    "    scaled_minutiae = []\n",
    "    for value in fingerprint_minutiae:\n",
    "            try:\n",
    "                # Attempt to convert directly to float\n",
    "                converted_value = float(value)\n",
    "                if np.isnan(converted_value):\n",
    "                    converted_value = 0  # Replace NaN with 0 or another appropriate value\n",
    "                scaled_minutiae.append(converted_value)\n",
    "            except ValueError:\n",
    "                # If direct conversion fails, try evaluating as a list and then convert each element\n",
    "                try:\n",
    "                    list_values = ast.literal_eval(value)\n",
    "                    if isinstance(list_values, list):\n",
    "                        scaled_list_values = [float(v) if not np.isnan(float(v)) else 0 for v in list_values]  # Replace NaN within the list\n",
    "                        scaled_minutiae.extend(scaled_list_values)\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "                except ValueError:\n",
    "                    # If conversion still fails, handle or report the erroneous value\n",
    "                    print(f\"Could not convert value: {value}\")\n",
    "\n",
    "    val = 0\n",
    "    target_length = 1000\n",
    "\n",
    "    while len(scaled_minutiae) < target_length:\n",
    "        scaled_minutiae.append(val)\n",
    "    arr = np.array(scaled_minutiae)\n",
    "    encrypted_vec = ts.ckks_vector(context, arr)\n",
    "    return encrypted_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrollment (entire database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Demo Purposes - these statements define the folder we will\n",
    "# enroll for storage in our 'database'\n",
    "\n",
    "folder_path = r\"C:\\Users\\quinn\\Desktop\\LiveDemo\\EnrollmentSet\"\n",
    "csv_filepath = r\"C:\\Users\\quinn\\Desktop\\LiveDemo\\database.csv\"\n",
    "target_minutiae_count = 200\n",
    "\n",
    "process_images_from_folder(folder_path, csv_filepath, target_minutiae_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID  Type  LocationX  LocationY          Orientation\n",
      "0      0     0         85        238             [-180.0]\n",
      "1      0     0         87        273               [-0.0]\n",
      "2      0     0         96        255               [-0.0]\n",
      "3      0     0        103        293   [153.434948822922]\n",
      "4      0     0        112        196  [-153.434948822922]\n",
      "...   ..   ...        ...        ...                  ...\n",
      "1395   6     1         -1         -1                  NaN\n",
      "1396   6     1         -1         -1                  NaN\n",
      "1397   6     1         -1         -1                  NaN\n",
      "1398   6     1         -1         -1                  NaN\n",
      "1399   6     1         -1         -1                  NaN\n",
      "\n",
      "[1400 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('database.csv')\n",
    "df\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating TenSEAL context\n",
    "\n",
    "# - Contains public key, private key, and homomorphic context\n",
    "#      for doing computations (add, mult, matrix)\n",
    "#\n",
    "# - Defines the scheme (CKKS), the degree to which the polynomial\n",
    "#      can calculate floating point values, the bit sizes of \n",
    "#      coefficients values in the polynomial, and the encryption type\n",
    "#      \n",
    "# - poly_modulus_degree affects the following:\n",
    "#       1) Number of coefficients in plaintext polynomials\n",
    "#       2) Size of ciphertext elements\n",
    "#       3) Computational performance (big=bad!)\n",
    "#       4) Security level (big=good!)\n",
    "#\n",
    "# - coeff_mod_bit_sizes refers to a list of primes (coefficient modulus)\n",
    "#       which affects the following:\n",
    "#       1) Size of ciphertext elements\n",
    "#       2) Number of encrypted multiplications supported\n",
    "#       3) The security level (big=bad)\n",
    "#       Note: Each of the prime numbers in the coeff. modulus must be\n",
    "#             at most 60bits and congruent to 1 mod 2*poly_modulus_degree\n",
    "\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=16384,\n",
    "    coeff_mod_bit_sizes=[60, 40, 40, 40, 60],\n",
    "    encryption_type=ts.ENCRYPTION_TYPE.ASYMMETRIC\n",
    ")\n",
    "\n",
    "context.global_scale = 2**40\n",
    "context.generate_galois_keys()\n",
    "context.generate_relin_keys()\n",
    "secret_key = context.secret_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This statement is encrypting every entry in the dataframe\n",
    "# and saving it as a dictionary, with ID: Data as the format\n",
    "\n",
    "encrypted_database = encryptPrintsDatabase(df, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a simple method to save the database for \n",
    "# server storage or transfer. Other more sophisticated\n",
    "# methods would be desired for actual cloud storage and \n",
    "# computations.\n",
    "\n",
    "pickle_dict = {}\n",
    "for i in range(0, 7):\n",
    "    buff = encrypted_database[i].serialize()\n",
    "    pickle_dict[i] = buff\n",
    "with open('saved_database.pkl', 'wb') as f:\n",
    "    pickle.dump(pickle_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the last block, these statements show how the \n",
    "# public key and private key can be saved to securely\n",
    "# transfer to a trusted party for decryption of \n",
    "# computation results.\n",
    "\n",
    "\n",
    "public_key_pickle = context.serialize(save_secret_key=False)\n",
    "private_key_pickle = context.serialize(save_secret_key=True)\n",
    "\n",
    "with open('public_key.key', 'wb') as key_file:\n",
    "    key_file.write(public_key_pickle)\n",
    "\n",
    "with open('private_key.key', 'wb') as pkey_file:\n",
    "    pkey_file.write(private_key_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These statements show the database and PUBLIC key\n",
    "# being loaded from the previously saved file, and the\n",
    "# database being 'recontextualized' after file transfer\n",
    "\n",
    "with open('saved_database.pkl', 'rb') as f:\n",
    "    loaded_pickle_dict = pickle.load(f)\n",
    "\n",
    "with open('public_key.key', 'rb') as k:\n",
    "    public_context_dat = k.read()\n",
    "    public_context = ts.context_from(public_context_dat)\n",
    "    \n",
    "new_vectors = {}\n",
    "for i, serialized_data in loaded_pickle_dict.items():\n",
    "    vector = ts.lazy_ckks_vector_from(serialized_data)\n",
    "    vector.link_context(public_context)\n",
    "    new_vectors[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing homomorphic encrypted calculations\n",
    "\n",
    "vector_subtraction = new_vectors[1] - new_vectors[2]\n",
    "vector_multiplication = new_vectors[1]*new_vectors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the image we will use to verify matching data\n",
    "\n",
    "img_path = r\"C:\\Users\\quinn\\Desktop\\LiveDemo\\AuthenticationSet\\76\\076_3_5.tif\"\n",
    "client_print = process_image_to_dataframe(1, img_path, \"client.csv\", 200)\n",
    "client_print\n",
    "\n",
    "encrypted_client = encryptSinglePrint(client_print, 1, public_context)\n",
    "\n",
    "buff = encrypted_client.serialize()\n",
    "request = {\"1\":buff}\n",
    "with open('saved_client.pkl', 'wb') as f:\n",
    "    pickle.dump(request, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.959724637379424e-06]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('saved_client.pkl', 'rb') as r:\n",
    "    loaded_client = pickle.load(r)\n",
    "\n",
    "client_data = {}\n",
    "for i, serialized_data in loaded_client.items():\n",
    "    vector = ts.lazy_ckks_vector_from(serialized_data)\n",
    "    vector.link_context(public_context)\n",
    "    client_data[i] = vector\n",
    "\n",
    "distance_squared = (new_vectors[6] - client_data['1']).dot(new_vectors[6] - client_data['1'])\n",
    "\n",
    "with open('private_key.key', 'rb') as pk:\n",
    "    secret_context_dat = pk.read()\n",
    "    secret_context = ts.context_from(secret_context_dat)\n",
    "\n",
    "distance_squared.decrypt(secret_context.secret_key())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
