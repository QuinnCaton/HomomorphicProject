{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tenseal as ts\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import fingerprint_feature_extractor\n",
    "import fingerprint_enhancer\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method was used in early tests to randomly generate data in our desired format\n",
    "def generate_minutiae(num_fingerprints, num_minutiae_per_fingerprint):\n",
    "    dataset = []\n",
    "    for i in range(num_fingerprints):\n",
    "        fingerprint = []\n",
    "        fingerprint_id = i+1\n",
    "        for _ in range(num_minutiae_per_fingerprint):\n",
    "            x = random.randint(0, 500) # x coord\n",
    "            y = random.randint(0, 500) # y coord\n",
    "            angle = random.randint(0, 360) # angle of minutiae\n",
    "            minutiae_type = random.randint(0, 1) #0 for ridge, 1 for bif\n",
    "            fingerprint.append((fingerprint_id, x, y, angle, minutiae_type)) # add to individual fingerprint data\n",
    "        dataset.append(fingerprint)\n",
    "    return dataset\n",
    "\n",
    "# Saves minutiae points to .csv file for ease of use later\n",
    "def save_to_csv(dataset, filename):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['ID', 'X', 'Y', 'Orientation', 'Type'])\n",
    "        for fingerprint in dataset:\n",
    "            for minutiae in fingerprint:\n",
    "                writer.writerow(minutiae)\n",
    "\n",
    "# UNUSED - Encrypts a dataframe as a dictionary, with ID maintained but minutiae encrypted\n",
    "def encrypt_fingerprint_data(df, context):\n",
    "    encrypted_minutiae_dict = {}\n",
    "    for fingerprint_id in df['ID'].unique():\n",
    "        fingerprint_minutiae = df[df['ID'] == fingerprint_id][['X', 'Y', 'Orientation', 'Type']].values.flatten().tolist()\n",
    "        scaled_minutiae = [float(value) for value in fingerprint_minutiae]\n",
    "        encrypted_minutiae = ts.ckks_vector(context, scaled_minutiae)\n",
    "        encrypted_minutiae_dict[fingerprint_id]=encrypted_minutiae\n",
    "    return encrypted_minutiae_dict\n",
    "\n",
    "# Unused concept of finding a specific match in a database\n",
    "def find_matching_fingerprint_id(encrypted_query, encrypted_data, context):\n",
    "    min_distance = float('inf')\n",
    "    matching_id = None\n",
    "\n",
    "    for fingerprint_id, encrypted_minutiae in encrypted_data.items():\n",
    "        difference_vector = encrypted_query - encrypted_minutiae\n",
    "        distance_squared = difference_vector.dot(difference_vector)\n",
    "\n",
    "        distance_squared_decrypted = distance_squared.decrypt()\n",
    "        distance = np.sqrt(distance_squared_decrypted)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            matching_id = fingerprint_id\n",
    "\n",
    "    return matching_id\n",
    "\n",
    "# Unused method for normalizing values\n",
    "def normalize_and_scale_minutiae(df):\n",
    "    df['locX'] = (df['locX'] / df['locX'].max())\n",
    "    df['locY'] = (df['locY'] / df['locY'].max())\n",
    "    #df['Orientation'] = df['Orientation'] / 360\n",
    "    # No change for 'Type' as it's categorical (0 or 1)\n",
    "    return df\n",
    "\n",
    "# Used to pad the dataframe for compatibility with tensor operations\n",
    "def pad_or_truncate_minutiae_list(minutiaeList, targetLength):\n",
    "    # Define dummy minutiae with impossible coordinates and orientation as placeholder\n",
    "    dummy_minutiae = fingerprint_feature_extractor.MinutiaeFeature(-1, -1, float('nan'), 'Dummy')\n",
    "    currentLength = len(minutiaeList)\n",
    "    \n",
    "    if currentLength > targetLength:\n",
    "        return minutiaeList[:targetLength]\n",
    "    else:\n",
    "        return minutiaeList + [dummy_minutiae] * (targetLength - currentLength)\n",
    "\n",
    "# Used for preprocessing a folder of fingerprint images\n",
    "def process_images_from_folder(folder_path, csv_filepath, target_minutiae_count, enrollment = True):\n",
    "    headers = ['ID', 'Type', 'LocationX', 'LocationY', 'Orientation']\n",
    "    \n",
    "    with open(csv_filepath, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        ID = 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".tif\"):  # Check for TIFF images, adjust if using other formats\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = cv2.imread(img_path, 0)\n",
    "                out = fingerprint_enhancer.enhance_Fingerprint(img)\n",
    "                \n",
    "                # Process image to enhance and extract features, replace with your actual method\n",
    "                # out = fingerprint_enhancer.enhance_Fingerprint(img)\n",
    "                if(enrollment==False):\n",
    "                    cv2.imshow('enhanced_image', out);\n",
    "                    cv2.waitKey(0)\n",
    "                    FeaturesTerminations, FeaturesBifurcations = fingerprint_feature_extractor.extract_minutiae_features(\n",
    "                                    out, spuriousMinutiaeThresh=10, invertImage=False, showResult=True, saveResult=True)\n",
    "                else:\n",
    "                    FeaturesTerminations, FeaturesBifurcations = fingerprint_feature_extractor.extract_minutiae_features(\n",
    "                                    out, spuriousMinutiaeThresh=10, invertImage=False, showResult=False, saveResult=True)\n",
    "                \n",
    "                # Ensure a fixed number of minutiae\n",
    "                FeaturesTerminations = pad_or_truncate_minutiae_list(FeaturesTerminations, target_minutiae_count//2)\n",
    "                FeaturesBifurcations = pad_or_truncate_minutiae_list(FeaturesBifurcations, target_minutiae_count//2)\n",
    "\n",
    "                # Write termination features\n",
    "                for feature in FeaturesTerminations:\n",
    "                    writer.writerow([ID, 0, feature.locX, feature.locY, feature.Orientation])\n",
    "                \n",
    "                # Write bifurcation features\n",
    "                for feature in FeaturesBifurcations:\n",
    "                    writer.writerow([ID, 1, feature.locX, feature.locY, feature.Orientation])\n",
    "            ID+=1\n",
    "\n",
    "# Used to encrypt the whole dataframe to dictionary form\n",
    "# ID: (encrypted data)\n",
    "def encryptPrintsDatabase(df, context):\n",
    "    total_prints = len(pd.unique(df['ID']))\n",
    "    database = {}\n",
    "    for i in range(0, total_prints):\n",
    "        fingerprint_minutiae = df[df['ID'] == i][['LocationX', 'LocationY', 'Orientation', 'Type']].values.flatten().tolist()\n",
    "        scaled_minutiae = []\n",
    "\n",
    "        for value in fingerprint_minutiae:\n",
    "            try:\n",
    "                # Attempt to convert directly to float\n",
    "                converted_value = float(value)\n",
    "                if np.isnan(converted_value):\n",
    "                    converted_value = 0  # Replace NaN with 0 or another appropriate value\n",
    "                scaled_minutiae.append(converted_value)\n",
    "            except ValueError:\n",
    "                # If direct conversion fails, try evaluating as a list and then convert each element\n",
    "                try:\n",
    "                    list_values = ast.literal_eval(value)\n",
    "                    if isinstance(list_values, list):\n",
    "                        scaled_list_values = [float(v) if not np.isnan(float(v)) else 0 for v in list_values]  # Replace NaN within the list\n",
    "                        scaled_minutiae.extend(scaled_list_values)\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "                except ValueError:\n",
    "                    # If conversion still fails, handle or report the erroneous value\n",
    "                    print(f\"Could not convert value: {value}\")\n",
    "\n",
    "        val = 0\n",
    "        target_length = 1000\n",
    "\n",
    "        while len(scaled_minutiae) < target_length:\n",
    "            scaled_minutiae.append(val)\n",
    "        arr = np.array(scaled_minutiae)\n",
    "        enc = ts.ckks_vector(context, arr)\n",
    "        database[i] = enc\n",
    "    \n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_to_dataframe(ID, img_path, file_path, target_minutiae_count):\n",
    "    headers = ['ID', 'Type', 'LocationX', 'LocationY', 'Orientation']\n",
    "    \n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        out = fingerprint_enhancer.enhance_Fingerprint(img)\n",
    "        cv2.imshow('enhanced_image', out);\n",
    "        cv2.waitKey(0)\n",
    "        # Extract features\n",
    "        FeaturesTerminations, FeaturesBifurcations = fingerprint_feature_extractor.extract_minutiae_features(\n",
    "                        out, spuriousMinutiaeThresh=10, invertImage=False, showResult=True, saveResult=True)\n",
    "\n",
    "        FeaturesTerminations = pad_or_truncate_minutiae_list(FeaturesTerminations, target_minutiae_count//2)\n",
    "        FeaturesBifurcations = pad_or_truncate_minutiae_list(FeaturesBifurcations, target_minutiae_count//2)\n",
    "\n",
    "    # Write termination features\n",
    "        for feature in FeaturesTerminations:\n",
    "            writer.writerow([ID, 0, feature.locX, feature.locY, feature.Orientation])\n",
    "        \n",
    "        # Write bifurcation features\n",
    "        for feature in FeaturesBifurcations:\n",
    "            writer.writerow([ID, 1, feature.locX, feature.locY, feature.Orientation])\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def encryptSinglePrint(df, ID, context):\n",
    "    fingerprint_minutiae = df[df['ID'] == ID][['LocationX', 'LocationY', 'Orientation', 'Type']].values.flatten().tolist()\n",
    "    scaled_minutiae = []\n",
    "    for value in fingerprint_minutiae:\n",
    "            try:\n",
    "                # Attempt to convert directly to float\n",
    "                converted_value = float(value)\n",
    "                if np.isnan(converted_value):\n",
    "                    converted_value = 0  # Replace NaN with 0 or another appropriate value\n",
    "                scaled_minutiae.append(converted_value)\n",
    "            except ValueError:\n",
    "                # If direct conversion fails, try evaluating as a list and then convert each element\n",
    "                try:\n",
    "                    list_values = ast.literal_eval(value)\n",
    "                    if isinstance(list_values, list):\n",
    "                        scaled_list_values = [float(v) if not np.isnan(float(v)) else 0 for v in list_values]  # Replace NaN within the list\n",
    "                        scaled_minutiae.extend(scaled_list_values)\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "                except ValueError:\n",
    "                    # If conversion still fails, handle or report the erroneous value\n",
    "                    print(f\"Could not convert value: {value}\")\n",
    "\n",
    "    val = 0\n",
    "    target_length = 1000\n",
    "\n",
    "    while len(scaled_minutiae) < target_length:\n",
    "        scaled_minutiae.append(val)\n",
    "    arr = np.array(scaled_minutiae)\n",
    "    encrypted_vec = ts.ckks_vector(context, arr)\n",
    "    return encrypted_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrollment (entire database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Demo Purposes - these statements define the folder we will\n",
    "# enroll for storage in our 'database'\n",
    "\n",
    "folder_path = r\"C:\\Users\\quinn\\Desktop\\LiveDemo\\EnrollmentSet\"\n",
    "csv_filepath = r\"C:\\Users\\quinn\\Desktop\\LiveDemo\\database.csv\"\n",
    "target_minutiae_count = 200\n",
    "\n",
    "process_images_from_folder(folder_path, csv_filepath, target_minutiae_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID  Type  LocationX  LocationY          Orientation\n",
      "0      0     0         85        238             [-180.0]\n",
      "1      0     0         87        273               [-0.0]\n",
      "2      0     0         96        255               [-0.0]\n",
      "3      0     0        103        293   [153.434948822922]\n",
      "4      0     0        112        196  [-153.434948822922]\n",
      "...   ..   ...        ...        ...                  ...\n",
      "1395   6     1         -1         -1                  NaN\n",
      "1396   6     1         -1         -1                  NaN\n",
      "1397   6     1         -1         -1                  NaN\n",
      "1398   6     1         -1         -1                  NaN\n",
      "1399   6     1         -1         -1                  NaN\n",
      "\n",
      "[1400 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\quinn\\Desktop\\LiveDemo\\database.csv\")\n",
    "df\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating TenSEAL context\n",
    "\n",
    "# - Contains public key, private key, and homomorphic context\n",
    "#      for doing computations (add, mult, matrix)\n",
    "#\n",
    "# - Defines the scheme (CKKS), the degree to which the polynomial\n",
    "#      can calculate floating point values, the bit sizes of \n",
    "#      coefficients values in the polynomial, and the encryption type\n",
    "#      \n",
    "# - poly_modulus_degree affects the following:\n",
    "#       1) Number of coefficients in plaintext polynomials\n",
    "#       2) Size of ciphertext elements\n",
    "#       3) Computational performance (big=bad!)\n",
    "#       4) Security level (big=good!)\n",
    "#\n",
    "# - coeff_mod_bit_sizes refers to a list of primes (coefficient modulus)\n",
    "#       which affects the following:\n",
    "#       1) Size of ciphertext elements\n",
    "#       2) Number of encrypted multiplications supported\n",
    "#       3) The security level (big=bad)\n",
    "#       Note: Each of the prime numbers in the coeff. modulus must be\n",
    "#             at most 60bits and congruent to 1 mod 2*poly_modulus_degree\n",
    "\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=16384,\n",
    "    coeff_mod_bit_sizes=[60, 40, 40, 40, 60],\n",
    "    encryption_type=ts.ENCRYPTION_TYPE.ASYMMETRIC\n",
    ")\n",
    "\n",
    "context.global_scale = 2**40\n",
    "context.generate_galois_keys()\n",
    "context.generate_relin_keys()\n",
    "secret_key = context.secret_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This statement is encrypting every entry in the dataframe\n",
    "# and saving it as a dictionary, with ID: Data as the format\n",
    "\n",
    "encrypted_database = encryptPrintsDatabase(df, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is a simple method to save the database for \n",
    "# server storage or transfer. Other more sophisticated\n",
    "# methods would be desired for actual cloud storage and \n",
    "# computations.\n",
    "\n",
    "pickle_dict = {}\n",
    "for i in range(0, 7):\n",
    "    buff = encrypted_database[i].serialize()\n",
    "    pickle_dict[i] = buff\n",
    "with open('saved_database.pkl', 'wb') as f:\n",
    "    pickle.dump(pickle_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the last block, these statements show how the \n",
    "# public key and private key can be saved to securely\n",
    "# transfer to a trusted party for decryption of \n",
    "# computation results.\n",
    "\n",
    "\n",
    "public_key_pickle = context.serialize(save_secret_key=False)\n",
    "private_key_pickle = context.serialize(save_secret_key=True)\n",
    "\n",
    "with open('public_key.key', 'wb') as key_file:\n",
    "    key_file.write(public_key_pickle)\n",
    "\n",
    "with open('private_key.key', 'wb') as pkey_file:\n",
    "    pkey_file.write(private_key_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These statements show the database and PUBLIC key\n",
    "# being loaded from the previously saved file, and the\n",
    "# database being 'recontextualized' after file transfer\n",
    "\n",
    "with open('saved_database.pkl', 'rb') as f:\n",
    "    loaded_pickle_dict = pickle.load(f)\n",
    "\n",
    "with open('public_key.key', 'rb') as k:\n",
    "    public_context_dat = k.read()\n",
    "    public_context = ts.context_from(public_context_dat)\n",
    "    \n",
    "new_vectors = {}\n",
    "for i, serialized_data in loaded_pickle_dict.items():\n",
    "    vector = ts.lazy_ckks_vector_from(serialized_data)\n",
    "    vector.link_context(public_context)\n",
    "    new_vectors[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tenseal.tensors.ckksvector.CKKSVector object at 0x0000025A22ED5400>\n",
      "<tenseal.tensors.ckksvector.CKKSVector object at 0x0000025A22EDAB80>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'secret_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(sub)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(mult)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sub\u001b[38;5;241m.\u001b[39mdecrypt(\u001b[43msecret_context\u001b[49m\u001b[38;5;241m.\u001b[39msecret_key()))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(mult\u001b[38;5;241m.\u001b[39mdecrypt(secret_context\u001b[38;5;241m.\u001b[39msecret_key()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'secret_context' is not defined"
     ]
    }
   ],
   "source": [
    "# Performing homomorphic encrypted calculations\n",
    "# Two example: subtraction and multiplication, yields encrypted vectors\n",
    "# If decrypted, they show the corresponding values\n",
    "sub = new_vectors[1] - new_vectors[2]\n",
    "mult = new_vectors[1]*new_vectors[2]\n",
    "print(sub)\n",
    "print(mult)\n",
    "\n",
    "print(sub.decrypt(secret_context.secret_key()))\n",
    "print(mult.decrypt(secret_context.secret_key()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the image we will use to verify matching data\n",
    "\n",
    "img_path = r\"C:\\Users\\quinn\\Desktop\\LiveDemo\\AuthenticationSet\\76\\076_3_5.tif\"\n",
    "client_print = process_image_to_dataframe(1, img_path, \"client.csv\", 200)\n",
    "client_print\n",
    "\n",
    "encrypted_client = encryptSinglePrint(client_print, 1, public_context)\n",
    "\n",
    "# These lines allow for saving and transferring of encrypted data\n",
    "buff = encrypted_client.serialize()\n",
    "request = {\"1\":buff}\n",
    "with open('saved_client.pkl', 'wb') as f:\n",
    "    pickle.dump(request, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between points in successful verification: [1.749625275640262e+30]\n",
      "Distance between points in unsuccessful verification: [5.117389056009587e+31]\n"
     ]
    }
   ],
   "source": [
    "# Loading encrypted client data\n",
    "with open('saved_client.pkl', 'rb') as r:\n",
    "    loaded_client = pickle.load(r)\n",
    "\n",
    "# A process called recontextualizing, after storage\n",
    "client_data = {}\n",
    "for i, serialized_data in loaded_client.items():\n",
    "    vector = ts.lazy_ckks_vector_from(serialized_data)\n",
    "    vector.link_context(public_context)\n",
    "    client_data[i] = vector\n",
    "\n",
    "# Loading the private context/key from storage\n",
    "with open('private_key.key', 'rb') as pk:\n",
    "    secret_context_dat = pk.read()\n",
    "    secret_context = ts.context_from(secret_context_dat)\n",
    "\n",
    "# Distance between minutiae points of the saved data and query data\n",
    "distance_squared = (new_vectors[6] - client_data['1']).dot(new_vectors[6] - client_data['1'])\n",
    "verify_dist = distance_squared.decrypt(secret_context.secret_key())\n",
    "\n",
    "print(\"Distance between points in successful verification:\", verify_dist)\n",
    "\n",
    "distance_squared2 = (new_vectors[5] - client_data['1']).dot(new_vectors[5] - client_data['1'])\n",
    "verify_dist2 = distance_squared2.decrypt(secret_context.secret_key())\n",
    "\n",
    "print(\"Distance between points in unsuccessful verification:\", verify_dist2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
